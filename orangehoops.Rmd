---
title: 'Orange Hoops: Player Performance'
author: "Wei-Chieh Chen Peiya Qi Yash Gaikwad"
date: "2024-11-13"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(glmnet)
library(caret)
library(Matrix)
library(dplyr)
library(tidyverse)
library(ggplot2)
library(corrplot)
library(readr)
library(Metrics)
library(pROC)
library(pls)  # For PLSR and PCR models


# Load and filter data
performance <- read.csv("Data/PBP2324.csv", sep = ",", header = TRUE)

# Define clutch time: last 5 minutes of the second half with score difference between -5 and 5
clutch_data <- performance %>%
  filter(
    half == 2,
    secs_remaining <= 300,
    score_diff >= -5 & score_diff <= 5,
    shot_team == "Purdue"  # Filter for shots made by Purdue
  )

# Drop rows with any NA values in relevant columns for modeling
clutch_data <- clutch_data %>%
  drop_na(secs_remaining, score_diff, shot_outcome, shooter, three_pt, free_throw, possession_before, possession_after, win_prob)

# Create shot success as the outcome variable
clutch_data$shot_success <- ifelse(clutch_data$shot_outcome == "made", 1, 0)

# Display data structure
str(clutch_data)
```

## Abstract
This project leverages NCAA Men’s Basketball data from the 2023-2024 season to identify Zach Edey as the optimal Purdue Men’s Basketball player for taking game-deciding shots during clutch moments. Clutch situations are defined as the last five minutes of the second half with a score differential within five points. Using an array of machine learning models, including Logistic Regression, Ridge Regression, Support Vector Machine (SVM), and ensemble methods, SVM emerged as the top-performing model, achieving 66.7% accuracy. The final SVM model recommends players for clutch scenarios, with Edey emerging as the top choice based on his consistently high shot success probabilities across various game situations.

## Introduction
In NCAA basketball, the final moments of close games are pivotal, and deciding which player should take the last shot can significantly impact the outcome. This study aims to offer a data-driven recommendation for the optimal player on the Purdue Men’s Basketball team to take a winning shot in high-pressure moments. By analyzing player performance under specific game conditions—such as time remaining, score differential, and possession—the project seeks to develop a reliable model to predict shot success during clutch situations. This model will ultimately provide actionable insights for in-game decision-making.

## Data Preparation
Clutch-time data from the NCAA Men’s Basketball Division 1 2023-2024 season was filtered for analysis, focusing on situations in the final five minutes of the game where the score difference was within five points. Variables included score_diff, secs_remaining, win_prob, shooter, and others related to game conditions and player actions. The outcome variable, shot_success, was binary, marking each shot as "Success" or "Fail".
```{r prepare}
# Define clutch time: last 10 minutes of the second half with score difference between -5 and 5
clutch_data <- performance %>%
  filter(
    half == 2,
    secs_remaining <= 300,
    score_diff >= -5 & score_diff <= 5,
    shot_team == "Purdue"  # Filter for shots made by Purdue
  )

# Drop rows with any NA values in relevant columns for modeling
clutch_data <- clutch_data %>%
  drop_na(secs_remaining, score_diff, shot_outcome, shooter, three_pt, free_throw, possession_before, possession_after, win_prob)

# Create shot success as the outcome variable
clutch_data$shot_success <- ifelse(clutch_data$shot_outcome == "made", 1, 0)

# Display data structure
str(clutch_data)
```

## Method & Result

### Exploratory Data Analysis
The exploratory data analysis (EDA) provides visual insights into the factors influencing shot success under clutch conditions. The following visualizations were used to understand player performance and contextual factors:

1. Distribution of Shot Success by Shooter:
This bar plot shows the percentage of successful versus failed shots for each player. A higher proportion of success in any player's bar indicates a greater reliability in clutch situations. This plot helps to quickly assess individual players' shot success rates, which can guide decisions on which players are most effective in scoring under pressure.
```{r explore}
# Plot the shot success rate per shooter
ggplot(clutch_data, aes(x = shooter, fill = as.factor(shot_success))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Shot Success Rate by Shooter", x = "Shooter", y = "Shot Success Rate (%)", fill = "Shot Outcome") +
  theme_minimal()

```

2. Distribution of Three-Point Shots and Free Throws:
The plots for Three-Point Attempt Success Rate and Free Throw Success Rate provide insights into players’ strengths with specific types of shots. A high success rate in the three-point plot indicates a player who is effective from beyond the arc, while a high success rate in the free throw plot highlights players who are reliable at the line. This breakdown is useful for understanding which shot types maximize each player’s scoring potential.
```{r explore2}
# Plot the distribution of three-point attempts and free throws
ggplot(clutch_data, aes(x = three_pt, fill = as.factor(shot_success))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Three-Point Attempt Success Rate", x = "Three-Point Attempt", y = "Shot Success Rate (%)", fill = "Shot Outcome") +
  theme_minimal()

ggplot(clutch_data, aes(x = free_throw, fill = as.factor(shot_success))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = scales::percent) +
  labs(title = "Free Throw Success Rate", x = "Free Throw", y = "Shot Success Rate (%)", fill = "Shot Outcome") +
  theme_minimal()
```

3. Time Remaining and Shot Success:
This scatter plot, with a trend line, shows how shot success probability changes as time counts down in the last five minutes of the game. If the trend line remains steady or increases, it indicates that players maintain or improve shot success as time decreases, showcasing resilience under time pressure. Conversely, a downward trend could indicate decreasing accuracy as pressure builds, suggesting time sensitivity in shot effectiveness.
```{r explore3}
# Plot shot success over time remaining
ggplot(clutch_data, aes(x = secs_remaining, y = shot_success, color = shooter)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(title = "Shot Success by Time Remaining", x = "Seconds Remaining", y = "Shot Success Probability") +
  theme_minimal()

```

4. Shot Success by Score Difference:
In this plot, shot success is analyzed against the score difference between teams. A consistent trend line across different score differences suggests that players’ shooting accuracy is stable regardless of the game’s competitiveness. A steep slope could imply that players perform differently based on the closeness of the game, either thriving under pressure or becoming less effective as games get tighter.
```{r explore4}
# Plot shot success by score difference
ggplot(clutch_data, aes(x = score_diff, y = shot_success, color = shooter)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(title = "Shot Success by Score Difference", x = "Score Difference", y = "Shot Success Probability") +
  theme_minimal()
```

5. Shot Success Rate by Game Situation:
The Shot Success by Win Probability plot reveals how players perform relative to the predicted chances of their team winning. A stable trend here suggests that players’ effectiveness is independent of the game’s expected outcome, while variability could indicate that certain players perform better when the odds are either in or against their favor.

The Shot Success by Possession Before Shot plot examines how possession changes prior to the shot influence success. It may show whether a player is more likely to score following specific possession scenarios, such as a defensive rebound or steal, offering insights into the flow and rhythm that set up successful shots.
```{r explore5}
# Plot shot success by win probability
ggplot(clutch_data, aes(x = win_prob, y = shot_success, color = shooter)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(title = "Shot Success by Win Probability", x = "Win Probability", y = "Shot Success Probability") +
  theme_minimal()

# Shot success by possession before the shot
ggplot(clutch_data, aes(x = possession_before, y = shot_success, color = shooter)) +
  geom_point() +
  geom_smooth(method = "glm", method.args = list(family = "binomial"), se = FALSE) +
  labs(title = "Shot Success by Possession Before Shot", x = "Possession Before", y = "Shot Success Probability") +
  theme_minimal()

```

6. Feature Plot:
The density plots for each predictor variable allow for comparison between successful and failed shots across features like score difference, time remaining, and win probability. Peaks in the density plots for successful shots can reveal favorable conditions that correlate with higher shot success rates. This plot enables a nuanced understanding of the conditions under which players are most effective, helping to inform situational strategies based on these factors.
```{r feature}
# Prepare the dataset for feature plot
# Convert shot_success to a factor if it’s not already
clutch_data$shot_success <- as.factor(clutch_data$shot_success)

# Select predictors and the response variable
predictors <- clutch_data %>%
  select(score_diff, secs_remaining, attendance, naive_win_prob, 
         home_favored_by, total_line, home_time_out_remaining, 
         away_time_out_remaining, win_prob)

# Create the feature plot
featurePlot(
  x = predictors,
  y = clutch_data$shot_success,
  plot = "density",
  scales = list(x = list(relation = "free"), y = list(relation = "free")),
  auto.key = list(columns = 2),
  main = "Feature Plot of Predictors by Shot Success"
)
```

These EDA plots provided a foundation for selecting variables for model training, helping to capture essential aspects of player performance in clutch situations.

### Model Training and Model Comparison
Multiple machine learning models were trained to predict shot success in clutch moments, specifically in the last five minutes of a close game. These models included Logistic Regression, Ridge Regression, Support Vector Machine (SVM), Random Forest, Gradient Boosting Machine (GBM), and k-Nearest Neighbors (kNN). Each model was evaluated on performance metrics such as Accuracy, Kappa, Sensitivity (true positive rate), Specificity (true negative rate), AUC (Area Under the Curve), and F1 Score to assess its ability to distinguish between successful and unsuccessful shots under high-pressure scenarios.
```{r pressure, echo=TRUE}
# Load necessary libraries
library(caret)
library(pROC)

# Prepare Data
# Filter out rows where shooter is Caleb Furst
clutch_data <- subset(clutch_data, shooter != "Caleb Furst")

# Ensure outcome variable and shooter are factors, rename levels for clarity
clutch_data$shot_success <- factor(clutch_data$shot_success, levels = c(0, 1), labels = c("Fail", "Success"))
clutch_data$shooter <- as.factor(clutch_data$shooter)

# Remove any rows with missing values in the entire dataset
clutch_data <- na.omit(clutch_data)

# Check class balance to ensure both classes are present
if (any(table(clutch_data$shot_success) == 0)) {
  stop("Error: One of the classes (Fail or Success) has no records in the dataset.")
}

# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(clutch_data$shot_success, p = .8, list = FALSE)
train_data <- clutch_data[trainIndex, ]
test_data <- clutch_data[-trainIndex, ]

# Apply up-sampling on training data to balance classes if necessary
train_data <- upSample(x = train_data[, -which(names(train_data) == "shot_success")],
                       y = train_data$shot_success)
names(train_data)[ncol(train_data)] <- "shot_success"  # Rename outcome column

# Define a function to calculate metrics for a model
calculate_metrics <- function(model, test_data, positive_class = "Success") {
  pred <- predict(model, test_data)
  pred_prob <- predict(model, test_data, type = "prob")[, positive_class]
  cm <- confusionMatrix(pred, test_data$shot_success)
  auc_value <- auc(test_data$shot_success, pred_prob)
  f1 <- 2 * (cm$byClass["Pos Pred Value"] * cm$byClass["Sensitivity"]) / 
        (cm$byClass["Pos Pred Value"] + cm$byClass["Sensitivity"])
  
  list(Accuracy = cm$overall["Accuracy"], Kappa = cm$overall["Kappa"],
       Sensitivity = cm$byClass["Sensitivity"], Specificity = cm$byClass["Specificity"],
       AUC = auc_value, F1_Score = f1)
}

# Specify predictor formula using the selected features
predictors <- shot_success ~ score_diff + secs_remaining + attendance + naive_win_prob +
                            home_favored_by + total_line + home_time_out_remaining +
                            away_time_out_remaining + win_prob + shooter

# Train Models and Calculate Metrics

# Logistic Regression
set.seed(123)
logistic_model <- train(predictors, data = train_data, method = "glm", family = "binomial")
logistic_metrics <- calculate_metrics(logistic_model, test_data)

# Ridge Regression
set.seed(123)
ridge_model <- train(predictors, data = train_data, method = "glmnet",
                     tuneGrid = expand.grid(alpha = 0, lambda = seq(0.001, 0.1, by = 0.01)),
                     trControl = trainControl(method = "cv", number = 5, classProbs = TRUE))
ridge_metrics <- calculate_metrics(ridge_model, test_data)

# Support Vector Machine (SVM) with Radial Basis Function Kernel
set.seed(123)
svm_model <- train(predictors, data = train_data, method = "svmRadial", tuneLength = 10,
                   trControl = trainControl(method = "cv", number = 5, classProbs = TRUE))
svm_metrics <- calculate_metrics(svm_model, test_data)

# Random Forest
set.seed(123)
rf_model <- train(predictors, data = train_data, method = "rf", tuneLength = 5)
rf_metrics <- calculate_metrics(rf_model, test_data)

# Gradient Boosting Machine (GBM)
set.seed(123)
gbm_model <- train(predictors, data = train_data, method = "gbm", verbose = FALSE, tuneLength = 5,
                   trControl = trainControl(method = "cv", number = 5))
gbm_metrics <- calculate_metrics(gbm_model, test_data)

# K-Nearest Neighbors (KNN)
set.seed(123)
knn_model <- train(predictors, data = train_data, method = "knn", tuneLength = 5)
knn_metrics <- calculate_metrics(knn_model, test_data)

# Compile Results
results <- data.frame(
  Model = c("Logistic Regression", "Ridge Regression", "SVM", "Random Forest", "GBM", "KNN"),
  Accuracy = c(logistic_metrics$Accuracy, ridge_metrics$Accuracy, svm_metrics$Accuracy,
               rf_metrics$Accuracy, gbm_metrics$Accuracy, knn_metrics$Accuracy),
  Kappa = c(logistic_metrics$Kappa, ridge_metrics$Kappa, svm_metrics$Kappa,
            rf_metrics$Kappa, gbm_metrics$Kappa, knn_metrics$Kappa),
  Sensitivity = c(logistic_metrics$Sensitivity, ridge_metrics$Sensitivity, svm_metrics$Sensitivity,
                  rf_metrics$Sensitivity, gbm_metrics$Sensitivity, knn_metrics$Sensitivity),
  Specificity = c(logistic_metrics$Specificity, ridge_metrics$Specificity, svm_metrics$Specificity,
                  rf_metrics$Specificity, gbm_metrics$Specificity, knn_metrics$Specificity),
  AUC = c(logistic_metrics$AUC, ridge_metrics$AUC, svm_metrics$AUC,
          rf_metrics$AUC, gbm_metrics$AUC, knn_metrics$AUC),
  F1_Score = c(logistic_metrics$F1_Score, ridge_metrics$F1_Score, svm_metrics$F1_Score,
               rf_metrics$F1_Score, gbm_metrics$F1_Score, knn_metrics$F1_Score)
)

print(results)

```
Logistic Regression and Ridge Regression: These linear models offered baseline performance, with accuracies around 50-55.6%. While these models are simple and interpretable, their performance was relatively lower compared to non-linear models, suggesting limitations in capturing the complexity of clutch-time shot success.

Support Vector Machine (SVM): The SVM model, using a Radial Basis Function kernel, emerged as the top performer with an accuracy of 66.7%. It also scored high in AUC and F1 Score, indicating a strong balance between sensitivity and specificity. SVM's performance suggests it effectively handles non-linear patterns in the data, making it well-suited for this application.

Random Forest and GBM: These ensemble models achieved competitive accuracy rates (61-66.7%) and demonstrated robustness across different game situations. Although their performance was close to SVM, their slightly lower composite scores resulted in their ranking below SVM.

k-Nearest Neighbors (kNN): kNN’s performance was relatively lower, indicating that its simple distance-based approach was less effective in capturing the nuanced conditions of shot success.

### Selecting the Best Model
Metrics were normalized and weighted to calculate a Composite Score, providing an overall performance measure. Weights were assigned to Accuracy (30%), AUC (40%), and F1 Score (30%) to balance precision and predictive power. Based on the composite score, SVM ranked the highest, followed by Random Forest and GBM, making SVM the preferred model for player recommendations.
```{r bestmodel}
# Normalize metrics for comparison
results$Accuracy_norm <- results$Accuracy / max(results$Accuracy)
results$AUC_norm <- results$AUC / max(results$AUC)
results$F1_Score_norm <- results$F1_Score / max(results$F1_Score)

# Assign weights for composite scoring (adjust these as needed)
weight_accuracy <- 0.3
weight_auc <- 0.4
weight_f1 <- 0.3

# Calculate composite score
results$Composite_Score <- (results$Accuracy_norm * weight_accuracy) + 
                           (results$AUC_norm * weight_auc) + 
                           (results$F1_Score_norm * weight_f1)

# Sort by Composite_Score to find the best model
results <- results[order(-results$Composite_Score), ]
print("Ranked Models by Composite Score:")
print(results)

# Select the best model based on highest composite score
best_model <- results[1, ]
print("Best Model:")
print(best_model)

```
With its superior composite score, SVM was selected as the final model to drive recommendations. Its high accuracy and sensitivity to game-specific variables make it effective for identifying players with high shot success probabilities in clutch situations.

The Model Training and Comparison section shows that SVM outperformed other models, effectively predicting shot outcomes based on game conditions and player attributes. The structured comparison of models provides a clear rationale for selecting SVM, ensuring the recommendation system is backed by robust analysis and reliable metrics.

### Selecting the Optimal Player
The Optimal Player section analyzes the players' clutch-time shot performance to determine the best candidate for taking the final shot in high-stakes scenarios. The Support Vector Machine (SVM) model, selected for its superior accuracy and ability to handle non-linear data patterns, was used to predict each player’s likelihood of scoring under pressure.

The SVM model evaluated each player based on situational game factors such as score difference, time remaining, win probability, and other predictors. By assessing these conditions, the model calculated the probability of a successful shot for each player, ultimately identifying players with consistently high success rates in clutch scenarios.
```{r bestplayer}
# Load necessary libraries
library(caret)
library(pROC)
library(dplyr)

# Load and filter data
performance <- read.csv("Data/PBP2324.csv", sep = ",", header = TRUE)

# Define clutch time: last 5 minutes of the second half with score difference between -5 and 5
clutch_data <- performance %>%
  filter(
    half == 2,
    secs_remaining <= 300,
    score_diff >= -5 & score_diff <= 5,
    shot_team == "Purdue"  # Filter for shots made by Purdue
  )

# Drop rows with any NA values in relevant columns for modeling
clutch_data <- clutch_data %>%
  drop_na(secs_remaining, score_diff, shot_outcome, shooter, three_pt, free_throw, possession_before, possession_after, win_prob)

# Create shot success as the outcome variable
clutch_data$shot_success <- ifelse(clutch_data$shot_outcome == "made", 1, 0)

# Ensure outcome variable and shooter are factors
clutch_data$shot_success <- factor(clutch_data$shot_success, levels = c(0, 1), labels = c("Fail", "Success"))
clutch_data$shooter <- as.factor(clutch_data$shooter)

# Remove rows with missing values
clutch_data <- na.omit(clutch_data)

# Check class distribution
class_dist <- table(clutch_data$shot_success)
if (any(class_dist == 0)) {
  stop("Error: One of the classes (Fail or Success) has no records after filtering.")
}

# Split data into training and testing sets
set.seed(123)
trainIndex <- createDataPartition(clutch_data$shot_success, p = .8, list = FALSE)
train_data <- clutch_data[trainIndex, ]
test_data <- clutch_data[-trainIndex, ]

# Define the formula for the SVM model
predictors <- shot_success ~ score_diff + secs_remaining + attendance + naive_win_prob +
                            home_favored_by + total_line + home_time_out_remaining +
                            away_time_out_remaining + win_prob + shooter

# Train the SVM model
set.seed(123)
svm_model <- train(predictors, data = train_data, method = "svmRadial", tuneLength = 10,
                   trControl = trainControl(method = "cv", number = 5, classProbs = TRUE))

# Calculate probabilities for each player in the test data
test_data$predicted_prob <- predict(svm_model, test_data, type = "prob")[, "Success"]

# Choose the player with the highest predicted probability in each game situation
best_player_per_situation <- test_data %>%
  group_by(score_diff) %>%  # Replace `score_diff` with the relevant identifier for each situation
  filter(predicted_prob == max(predicted_prob)) %>%
  select(shooter, predicted_prob) %>%
  ungroup()

print(best_player_per_situation)

```

Based on the SVM model's predictions, Zach Eedy consistently achieved high probabilities of successful shots across various game situations. His performance data, when assessed against key predictors, demonstrated his reliability and scoring ability under pressure. Edey’s success rate in clutch-time moments set him apart from other players, making him the logical choice for critical, game-deciding shots. His consistency under varied game situations, as demonstrated by the SVM model’s analysis, validated him as the optimal player for this role.

The selection of Zach Edey is grounded in objective data analysis, showing that his success rate aligns with conditions typical of clutch scenarios (e.g., close score differences, limited time remaining). This data-backed approach ensures that the decision is not based on intuition alone but on statistical evidence of his ability to perform under pressure.

## Conclusion
This analysis highlights Zach Edey as the optimal player for Purdue Men’s Basketball in clutch-time situations, based on a predictive model that captures game-specific and player-specific attributes. The SVM model, with its high accuracy and adaptability to non-linear patterns, proved ideal for recommending players in high-pressure moments. This project demonstrates the value of machine learning in sports analytics, showcasing how data can enhance decision-making in critical scenarios.